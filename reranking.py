# -*- coding: utf-8 -*-
"""reranking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PBkBfJyQOSIpiiLHv03CMApLjrfKG2lo
"""







!pip install -U pymilvus

import pandas as pd
import torch
import re
import string
import numpy as np
import pandas as pd
from transformers import RobertaForMaskedLM, AutoTokenizer, AutoModel
from pymilvus import connections, Collection, MilvusClient, DataType

tokenizer_64 = AutoTokenizer.from_pretrained('./tokenizer_64')
model_64 = RobertaForMaskedLM.from_pretrained('./model_64')

tokenizer_128 = AutoTokenizer.from_pretrained('./tokenizer_128')
model_128 = RobertaForMaskedLM.from_pretrained('./model_128')


def get_embeddings(text, model, tokenizer, max_length):
    inputs = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=max_length,
        return_tensors="pt"
    )
    position_ids = torch.arange(inputs['input_ids'].shape[1], dtype=torch.long).unsqueeze(0)
    position_ids = position_ids.clamp(max=model.config.max_position_embeddings - 1)
    inputs['position_ids'] = position_ids

    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
        embeddings = outputs.hidden_states[-1]  # shape: (1, 48, hidden_dim)

        embeddings = embeddings.mean(dim=1).squeeze(0)

    return embeddings

def get_clean_text(name:str) -> str :

        PUNCT_TO_REMOVE = string.punctuation
        x=name
        if type(x) is str:

            # lowercase
            x = x.lower()
            # remove urls
            x = re.sub(r'(http|ftp|https)://([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:/~+#-]*[\w@?^=%&/~+#-])?', '', x)
            # remove htmls
            x = re.sub(r'<.*?>', '', x)
            # remove punctuations
            x = x.translate(str.maketrans('', '', PUNCT_TO_REMOVE))
            # remove â€™
            x = re.sub('\â€”','', x)

        return x


def clean_u_name(name: str) -> str:

    prefixes = ["MISS", "Miss", "miss", "LATE", "Late", "late", "LT", "lt",
    "SMT", "Smt", "smt", "MRS", "mrs", "MS", "ms", "Ms", "MST", "mst", "Mst"
    "KUMARI", "kumari", "Kumari", "DR", "Dr", "dr", "MR", "Mr", "mr"]

    pattern = r'^(?:' + '|'.join(prefixes) + r')\s+'

    # Remove prefixes
    while re.match(pattern, name, flags=re.IGNORECASE):
        name = re.sub(pattern, '', name, flags=re.IGNORECASE)

    return name.strip()

client = MilvusClient("milvus_demo_1.db")              # optional
collection_name = "demo_collection"

# def milvus_client_connect(url="http://localhost:19530", token="root:Milvus"):

#     client = MilvusClient(url=url,
#                           token=token)
#     return client

# client = milvus_client_connect()




# Create schema / table
schema = MilvusClient.create_schema(auto_id=False, enable_dynamic_field=True,)

# Add column/fields to table
# schema.add_field(field_name="id", datatype=DataType.INT64, is_primary=True, auto_id=True)
schema.add_field(field_name="applicant_id", datatype=DataType.VARCHAR, max_length=100,is_primary=True,auto_id=False)
schema.add_field(field_name="applicant_name", datatype=DataType.VARCHAR, max_length=100)
schema.add_field(field_name="applicant_dob", datatype=DataType.VARCHAR, max_length=100)
schema.add_field(field_name="father_name", datatype=DataType.VARCHAR, max_length=100)
schema.add_field(field_name="mother_name", datatype=DataType.VARCHAR, max_length=100)
#schema.add_field(field_name="total", datatype=DataType.VARCHAR, max_length=100)

schema.add_field(field_name="dense_embed_applicant_name", datatype=DataType.FLOAT_VECTOR, dim=64)
schema.add_field(field_name="dense_embed_father_name", datatype=DataType.FLOAT_VECTOR, dim=64)
schema.add_field(field_name="dense_embed_mother_name", datatype=DataType.FLOAT_VECTOR, dim=64)
schema.add_field(field_name="dense_embed_applicant_dob", datatype=DataType.FLOAT_VECTOR, dim=128)


# schema.add_field(field_name="dense_embed", datatype=DataType.FLOAT_VECTOR, dim=dimension,
#                   index_params={"index_type":"HNSW", "metric_type":"COSINE", "params": {"M": 32,
#                                                                                       "efConstruction": 100,
#                                                                                       "quantization": {
#                                                                                                           "type": "PQ",
#                                                                                                           "params": {
#                                                                                                                       "nbits": 8,
#                                                                                                                       "nlist": 100  # optional, depending on your PQ config
#                                                                                                                   }
#                                                                                                           }
#                                                                                       }
#                               }
#                   )





# Prepare index parameters
index_params = client.prepare_index_params()

# Add indexes
# index_params.add_index(field_name="id")
# index_params.add_index(field_name="applicant_id", index_type="INVERTED", )
index_params.add_index(field_name="applicant_name", index_type="INVERTED", )
index_params.add_index(field_name="applicant_dob", index_type="INVERTED", )
index_params.add_index(field_name="father_name", index_type="INVERTED", )
index_params.add_index(field_name="mother_name", index_type="INVERTED", )
#index_params.add_index(field_name="total", index_type="INVERTED", )

index_params.add_index(field_name="dense_embed_applicant_name", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},) # IVF FLAT
index_params.add_index(field_name="dense_embed_father_name", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},)
index_params.add_index(field_name="dense_embed_mother_name", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},)
index_params.add_index(field_name="dense_embed_applicant_dob", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},)


#index_params.add_index(field_name="dense_embed", index_type="HNSW", metric_type="COSINE")



if client.has_collection(collection_name=collection_name):
    client.drop_collection(collection_name=collection_name)      # "demo_collection"


client.create_collection(collection_name=collection_name,
                        schema=schema,
                        index_params=index_params,
                        )

df = pd.read_csv('kp_applicant_details1.csv', delimiter=';')

df['dense_embed_applicant_full_name'] = None
df['dense_embed_applicant_father_full_name'] = None
df['dense_embed_applicant_mother_full_name'] = None
df['dense_embed_applicant_dob'] = None

step_size = 200

for i in range(0, len(df), step_size):

    df_chunk = df.iloc[i:i+step_size]

    df.loc[df_chunk.index, 'applicant_full_name'] = df_chunk['applicant_full_name'].apply(lambda x: clean_u_name(get_clean_text(x)))
    df.loc[df_chunk.index, 'applicant_father_full_name'] = df_chunk['applicant_father_full_name'].apply(lambda x: clean_u_name(get_clean_text(x)))
    df.loc[df_chunk.index, 'applicant_mother_full_name'] = df_chunk['applicant_mother_full_name'].apply(lambda x: clean_u_name(get_clean_text(x)))
    df.loc[df_chunk.index, 'applicant_dob'] = df_chunk['applicant_dob'].apply(get_clean_text)



    texts_applicant_full_name = df.loc[df_chunk.index, 'applicant_full_name'].tolist()
    embeddings_applicant_full_name = get_embeddings(text=texts_applicant_full_name, model=model_64, tokenizer=tokenizer_64, max_length=48)

    for j, idx in enumerate(df_chunk.index):
        df.at[idx, 'dense_embed_applicant_full_name'] = embeddings_applicant_full_name[j].tolist()


    texts_applicant_father_full_name = df.loc[df_chunk.index, 'applicant_father_full_name'].tolist()
    embeddings_applicant_father_full_name = get_embeddings(text=texts_applicant_father_full_name, model=model_64, tokenizer=tokenizer_64, max_length=48)

    for j, idx in enumerate(df_chunk.index):
        df.at[idx, 'dense_embed_applicant_father_full_name'] = embeddings_applicant_father_full_name[j].tolist()


    texts_applicant_mother_full_name = df.loc[df_chunk.index, 'applicant_mother_full_name'].tolist()
    embeddings_applicant_mother_full_name = get_embeddings(text=texts_applicant_mother_full_name, model=model_64, tokenizer=tokenizer_64, max_length=48)

    for j, idx in enumerate(df_chunk.index):
        df.at[idx, 'dense_embed_applicant_mother_full_name'] = embeddings_applicant_mother_full_name[j].tolist()


    texts_applicant_dob = df.loc[df_chunk.index, 'applicant_dob'].tolist()
    embeddings_applicant_dob = get_embeddings(text=texts_applicant_dob, model=model_128, tokenizer=tokenizer_128, max_length=128)

    for j, idx in enumerate(df_chunk.index):
        df.at[idx, 'dense_embed_applicant_dob'] = embeddings_applicant_dob[j].tolist()



    # Milvus insert

    records = []

    for _, row in df_chunk.iterrows():

        record = {
                    "applicant_id": row['applicant_id'],
                    "applicant_name": row['applicant_full_name'],
                    "father_name": row['applicant_father_full_name'],
                    "mother_name": row['applicant_mother_full_name'],
                    "applicant_dob": row['applicant_dob'],
                    #"total": row['total'],
                    "dense_embed_applicant_name": row['dense_embed_applicant_full_name'],
                    "dense_embed_father_name": row['dense_embed_applicant_father_full_name'],
                    "dense_embed_mother_name": row['dense_embed_applicant_mother_full_name'],
                    "dense_embed_applicant_dob": row['dense_embed_applicant_dob'],
                }
        records.append(record)

    # Insert into Milvus
    client.insert(collection_name=collection_name, data=records)


print(client.describe_collection(collection_name=collection_name))

df.tail(2)





from pymilvus import AnnSearchRequest
from pymilvus import WeightedRanker, RRFRanker

def milvus_search(df, collection_name, threshold_milvus) :

  report= "milvus_results.csv"
  file = open(report, "a")
  file.write('applicant_id, applicant_name, father_name, mother_name, dob, matched_applicant_id, matched_applicant_name, matched_father_name, matched_mother_name, matched_dob, similarty_score\n')
  #file.write('applicant_id, matched_applicant_id, orginal, matched, similarty_score\n')
  filtered_lst = []

  for i in range(len(df)) :

    search_param_1 = {
                      "data": [df['dense_embed_applicant_full_name'].iloc[i]],
                      "anns_field": "dense_embed_applicant_name",
                      "param": {
                                "metric_type": "COSINE",
                                #"params": {"nprobe": 10}
                               },
                      "limit": 5
                    }
    request_1 = AnnSearchRequest(**search_param_1)

    search_param_2 = {
                      "data": [df['dense_embed_applicant_father_full_name'].iloc[i]],
                      "anns_field": "dense_embed_father_name",
                      "param": {
                                "metric_type": "COSINE",
                                #"params": {"nprobe": 10}
                               },
                      "limit": 5
                    }
    request_2 = AnnSearchRequest(**search_param_2)


    search_param_3 = {
                      "data": [df['dense_embed_applicant_mother_full_name'].iloc[i]],
                      "anns_field": "dense_embed_mother_name",
                      "param": {
                                "metric_type": "COSINE",
                                #"params": {"nprobe": 10}
                               },
                      "limit": 5
                    }
    request_3 = AnnSearchRequest(**search_param_3)


    search_param_4 = {
                      "data": [df['dense_embed_applicant_dob'].iloc[i]],
                      "anns_field": "dense_embed_applicant_dob",
                      "param": {
                                "metric_type": "COSINE",
                                #"params": {"nprobe": 10}
                               },
                      "limit": 5
                    }
    request_4 = AnnSearchRequest(**search_param_4)

    reqs = [request_1, request_2, request_3, request_4,]


    ranker = WeightedRanker(0.50, 0.15, 0.15, 0.20)
    #ranker = RRFRanker(100)


    results = client.hybrid_search(
                                    collection_name="demo_collection",
                                    reqs=reqs,
                                    search_params={
                                                   "params": {"ef":64},
                                                  },

                                    ranker=ranker,
                                    limit=5,
                                    output_fields=["applicant_id", "applicant_name", "father_name", "mother_name", "applicant_dob"]
                                  )


    for hits in results:

      for hit in hits:

        #if str(df['applicant_id'][i])!=hit.applicant_id and hit.distance>threshold_milvus:
        if hit.distance>threshold_milvus:

          print(f"{df['applicant_id'][i]}, {df['applicant_full_name'][i]}, {df['applicant_father_full_name'][i]}, {df['applicant_mother_full_name'][i]}, {df['applicant_dob'][i]}, {hit.applicant_id}, {hit.applicant_name}, {hit.father_name}, {hit.mother_name}, {hit.applicant_dob}, {hit.distance}")
          file.write(f"{df['applicant_id'][i]}, {df['applicant_full_name'][i]}, {df['applicant_father_full_name'][i]}, {df['applicant_mother_full_name'][i]}, {df['applicant_dob'][i]}, {hit.applicant_id}, {hit.applicant_name}, {hit.father_name}, {hit.mother_name}, {hit.applicant_dob}, {hit.distance}\n")

      print("-----------------------------")


  file.close()

filtered = milvus_search(df=df, collection_name="demo_collection", threshold_milvus=0.50)







"""...dob"""

import pandas as pd
import torch
import re
import string
import numpy as np
import pandas as pd
from transformers import RobertaForMaskedLM, AutoTokenizer, AutoModel
from pymilvus import connections, Collection, MilvusClient, DataType

tokenizer_128 = AutoTokenizer.from_pretrained('./tokenizer_128')
model_128 = RobertaForMaskedLM.from_pretrained('./model_128')

def get_embeddings(text, model, tokenizer, max_length):
    inputs = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=max_length,
        return_tensors="pt"
    )
    position_ids = torch.arange(inputs['input_ids'].shape[1], dtype=torch.long).unsqueeze(0)
    position_ids = position_ids.clamp(max=model.config.max_position_embeddings - 1)
    inputs['position_ids'] = position_ids

    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
        embeddings = outputs.hidden_states[-1]  # shape: (1, 48, hidden_dim)

        embeddings = embeddings.mean(dim=1).squeeze(0)

    return embeddings



def get_clean_text(name:str) -> str :

        PUNCT_TO_REMOVE = string.punctuation
        x=name
        if type(x) is str:

            # lowercase
            x = x.lower()
            # remove urls
            x = re.sub(r'(http|ftp|https)://([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:/~+#-]*[\w@?^=%&/~+#-])?', '', x)
            # remove htmls
            x = re.sub(r'<.*?>', '', x)
            # remove punctuations
            x = x.translate(str.maketrans('', '', PUNCT_TO_REMOVE))
            # remove â€™
            x = re.sub('\â€”','', x)

        return x


def clean_u_name(name: str) -> str:

    prefixes = ["MISS", "Miss", "miss", "LATE", "Late", "late", "LT", "lt",
    "SMT", "Smt", "smt", "MRS", "mrs", "MS", "ms", "Ms", "MST", "mst", "Mst"
    "KUMARI", "kumari", "Kumari", "DR", "Dr", "dr", "MR", "Mr", "mr"]

    pattern = r'^(?:' + '|'.join(prefixes) + r')\s+'

    # Remove prefixes
    while re.match(pattern, name, flags=re.IGNORECASE):
        name = re.sub(pattern, '', name, flags=re.IGNORECASE)

    return name.strip()

client = MilvusClient("milvus_demo_1.db")              # optional
collection_name = "demo_collection"

# def milvus_client_connect(url="http://localhost:19530", token="root:Milvus"):

#     client = MilvusClient(url=url,
#                           token=token)
#     return client

# client = milvus_client_connect()




# Create schema / table
schema = MilvusClient.create_schema(auto_id=False, enable_dynamic_field=True,)

# Add column/fields to table
# schema.add_field(field_name="id", datatype=DataType.INT64, is_primary=True, auto_id=True)
schema.add_field(field_name="applicant_id", datatype=DataType.VARCHAR, max_length=100,is_primary=True,auto_id=False)
schema.add_field(field_name="applicant_name", datatype=DataType.VARCHAR, max_length=100)
schema.add_field(field_name="applicant_dob", datatype=DataType.VARCHAR, max_length=100)
schema.add_field(field_name="father_name", datatype=DataType.VARCHAR, max_length=100)
schema.add_field(field_name="mother_name", datatype=DataType.VARCHAR, max_length=100)
#schema.add_field(field_name="total", datatype=DataType.VARCHAR, max_length=100)

# schema.add_field(field_name="dense_embed_applicant_name", datatype=DataType.FLOAT_VECTOR, dim=64)
# schema.add_field(field_name="dense_embed_father_name", datatype=DataType.FLOAT_VECTOR, dim=64)
# schema.add_field(field_name="dense_embed_mother_name", datatype=DataType.FLOAT_VECTOR, dim=64)
schema.add_field(field_name="dense_embed_applicant_dob", datatype=DataType.FLOAT_VECTOR, dim=128)


# Prepare index parameters
index_params = client.prepare_index_params()

# Add indexes
# index_params.add_index(field_name="id")
# index_params.add_index(field_name="applicant_id", index_type="INVERTED", )
index_params.add_index(field_name="applicant_name", index_type="INVERTED", )
index_params.add_index(field_name="applicant_dob", index_type="INVERTED", )
index_params.add_index(field_name="father_name", index_type="INVERTED", )
index_params.add_index(field_name="mother_name", index_type="INVERTED", )
#index_params.add_index(field_name="total", index_type="INVERTED", )

# index_params.add_index(field_name="dense_embed_applicant_name", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},) # IVF FLAT
# index_params.add_index(field_name="dense_embed_father_name", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},)
# index_params.add_index(field_name="dense_embed_mother_name", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},)
index_params.add_index(field_name="dense_embed_applicant_dob", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},)


#index_params.add_index(field_name="dense_embed", index_type="HNSW", metric_type="COSINE")



if client.has_collection(collection_name=collection_name):
    client.drop_collection(collection_name=collection_name)      # "demo_collection"


client.create_collection(collection_name=collection_name,
                        schema=schema,
                        index_params=index_params,
                        )



df = pd.read_csv('kp_applicant_details1.csv', delimiter=';')

# df['dense_embed_applicant_full_name'] = None
# df['dense_embed_applicant_father_full_name'] = None
# df['dense_embed_applicant_mother_full_name'] = None
df['dense_embed_applicant_dob'] = None

step_size = 200

for i in range(0, len(df), step_size):

    df_chunk = df.iloc[i:i+step_size]

    df.loc[df_chunk.index, 'applicant_full_name'] = df_chunk['applicant_full_name'].apply(lambda x: clean_u_name(get_clean_text(x)))
    df.loc[df_chunk.index, 'applicant_father_full_name'] = df_chunk['applicant_father_full_name'].apply(lambda x: clean_u_name(get_clean_text(x)))
    df.loc[df_chunk.index, 'applicant_mother_full_name'] = df_chunk['applicant_mother_full_name'].apply(lambda x: clean_u_name(get_clean_text(x)))
    df.loc[df_chunk.index, 'applicant_dob'] = df_chunk['applicant_dob'].apply(get_clean_text)


    texts_applicant_dob = df.loc[df_chunk.index, 'applicant_dob'].tolist()
    embeddings_applicant_dob = get_embeddings(text=texts_applicant_dob, model=model_128, tokenizer=tokenizer_128, max_length=128)

    for j, idx in enumerate(df_chunk.index):
        df.at[idx, 'dense_embed_applicant_dob'] = embeddings_applicant_dob[j].tolist()


    # Milvus insert

    records = []

    for _, row in df_chunk.iterrows():

        record = {
                    "applicant_id": row['applicant_id'],
                    "applicant_name": row['applicant_full_name'],
                    "father_name": row['applicant_father_full_name'],
                    "mother_name": row['applicant_mother_full_name'],
                    "applicant_dob": row['applicant_dob'],
                    # "total": row['total'],
                    # "dense_embed_applicant_name": row['dense_embed_applicant_full_name'],
                    # "dense_embed_father_name": row['dense_embed_applicant_father_full_name'],
                    # "dense_embed_mother_name": row['dense_embed_applicant_mother_full_name'],
                    "dense_embed_applicant_dob": row['dense_embed_applicant_dob'],
                }
        records.append(record)

    # Insert into Milvus
    client.insert(collection_name=collection_name, data=records)


print(client.describe_collection(collection_name=collection_name))

df.tail(2)

def milvus_search(df, collection_name, threshold_milvus) :

  report= "milvus_results.csv"
  file = open(report, "a")
  #file.write('applicant_id, matched_applicant_id, similarty_score\n')
  #file.write('applicant_id, matched_applicant_id, orginal, matched, similarty_score\n')
  file.write('applicant_id, matched_applicant_id, applicant_name, matched_applicant_name, applicant_dob, matched_applicant_dob, father_name, matched_father_name, mother_name, matched_mother_name, orginal, matched, similarty_score\n')
  filtered_lst = []

  for i in range(len(df)) :


    query_dense_vec = df['dense_embed_applicant_dob'].iloc[i]

    results = client.search(collection_name=collection_name,
                          data=[query_dense_vec],
                          anns_field="dense_embed_applicant_dob",


                          search_params={"params": {"ef":64},
                                        },

                          #search_params={"params": {"metric_type":"COSINE", "ef":1000}},     # HNSW



                          limit=5,
                          output_fields=["applicant_id", "applicant_name", "applicant_dob", "father_name", "mother_name"],
                          #filter=filter1,
                          #expr=expr1,
                          )


    for hits in results:
      # print(hits)
      # filtered = [hit for hit in hits if hit.distance > threshold_milvus]
      for hit in hits:


        if hit.distance>threshold_milvus:
          print(f"{df['applicant_id'][i]},{hit.applicant_id}, {df['applicant_full_name'][i]}, {hit.applicant_name}, {df['applicant_dob'][i]}, {hit.applicant_dob}, {df['applicant_father_full_name'][i]}, {hit.father_name}, {df['applicant_mother_full_name'][i]}, {hit.mother_name}, {hit.distance}")
          file.write(f"{df['applicant_id'][i]},{hit.applicant_id}, {df['applicant_full_name'][i]}, {hit.applicant_name}, {df['applicant_dob'][i]}, {hit.applicant_dob}, {df['applicant_father_full_name'][i]}, {hit.father_name}, {df['applicant_mother_full_name'][i]}, {hit.mother_name}, {hit.distance}\n")
      print("-----------------------------")

      # filtered_lst.append(filtered)

  # return filtered_lst
  file.close()

filtered = milvus_search(df=df, collection_name="demo_collection", threshold_milvus=0.50)



df = pd.read_csv('kp_applicant_details1.csv', delimiter=';')
df.to_csv("dob.csv")





!pip install --upgrade agno lancedb sentence-transformers transformers

from agno.embedder.sentence_transformer_embedder import SentenceTransformerEmbedder
from agno.models.huggingface import HuggingFace
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb, SearchType
from agno.agent import Agent

# 🧠 1. Load the embedding model (Hugging Face SentenceTransformer)
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

# 📥 2. Download PDF content manually (since agno.embedder expects wrapper)
url = "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
pdf_text = requests.get(url).content

# 📄 3. Save and extract plain text (optional: use pdfplumber or PyMuPDF)
with open("thai_recipes.pdf", "wb") as f:
    f.write(pdf_text)

# Optional: extract text if you want (using PyMuPDF or pdfplumber)
import fitz  # PyMuPDF
doc = fitz.open("thai_recipes.pdf")
full_text = "\n".join(page.get_text() for page in doc)
doc.close()

# 🔹 4. Chunk text and embed
chunks = chunk_text(full_text, chunk_size=512, overlap=64)
vectors = [embed_model.encode(chunk).tolist() for chunk in chunks]

# 💾 5. Store in LanceDB vector DB
vectordb = LanceDb(
    table_name="thai_recipes",
    uri="./tmp/lancedb",
    search_type=SearchType.vector
)
vectordb.create_collection()
vectordb.insert(texts=chunks, vectors=vectors)

# Create knowledge base from LanceDB
knowledge = TextKnowledgeBase(
    texts=chunks,
    vector_db=vectordb,
)

# 💬 7. Use open-source LLM from Hugging Face (phi, mistral, etc.)
chat_model = HuggingFaceChat(id="microsoft/phi-2")  # Small, Colab-friendly

# 🤖 8. Initialize Agent
agent = Agent(
    model=chat_model,
    knowledge=knowledge,
    add_references=True,
    search_knowledge=True,
    show_tool_calls=True,
    markdown=True,
)

# 🧪 9. Ask your question!
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup?", stream=True
)

from agno.agent import Agent
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

# Create a knowledge base of PDFs from URLs
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    # Use LanceDB as the vector database and store embeddings in the `recipes` table
    vector_db=LanceDb(
        table_name="recipes",
        uri="tmp/lancedb",
        search_type=SearchType.vector,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
# Load the knowledge base: Comment after first run as the knowledge base is already loaded
knowledge_base.load()

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    # Enable RAG by adding references from AgentKnowledge to the user prompt.
    add_references=True,
    # Set as False because Agents default to `search_knowledge=True`
    search_knowledge=False,
    show_tool_calls=True,
    markdown=True,
)
agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)

# Commented out IPython magic to ensure Python compatibility.
# %xterm

# Commented out IPython magic to ensure Python compatibility.
!git clone -b v2.0.0 https://github.com/milvus-io/milvus.git
# %cd milvus/core
!./ubuntu_build_deps.sh
!./build.sh -t Release







"""24-08-2025"""

!pip install pymilvus
!pip install rapidfuzz
!pip install pycryptodome

from pymilvus import MilvusClient, DataType, Collection, connections
from pymilvus import AnnSearchRequest
from pymilvus import WeightedRanker, RRFRanker
import numpy as np
import pandas as pd
import re
import string
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import torch
from rapidfuzz import distance as dist
#from encryption_decryption import data_security_gcm
from datetime import datetime

model = SentenceTransformer("all-MiniLM-L6-v2")

def process_name(name: str) -> str:


    #encrypt_decrypt_obj = data_security_gcm()

    # encryption (not required)
    #name = encrypt_decrypt_obj.encrypt(text=name)
    # print(name)

    # decryption
    #name = encrypt_decrypt_obj.decrypt(encr=name)
    # print(name)

    #####################################################

    # name = re.sub(r',', ' ', name)
    # name = re.sub(r'\.', ' ', name)

    name = re.sub(r'[,.()]', ' ', name)                # (remove but not replace with space)
    name = name.lower()


    #  if name has 2 or fewer words

    if len(name.split()) <= 2:

        PUNCT_TO_REMOVE = string.punctuation

        # Step 1: Lowercase
        name = name.lower()

        # Step 2: Remove digits, URLs, specific Unicode chars   (remove but not replace with space)
        name = re.sub(r'\d+', '', name)
        name = re.sub(r'!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~', '', name)
        name = re.sub(r'(http|ftp|https)://[\S]+', '', name)
        name = re.sub('â€”', '', name)

        # Step 3: Remove unwanted keywords (like "d/o", "co", etc.)
        name = re.sub(r'\b(d[^\w\s]?o|c[^\w\s]?o|u[^\w\s]?g|s[^\w\s]?o|w[^\w\s]?o|do|co|ug|so|fng|fngof|mng|mngof|mngf|d o|c o|u g|s o|minor|lti|l t i|ung)\b[^,]*', '', name, flags=re.IGNORECASE)

        # Step 5: Remove punctuation and extra spaces
        name = re.sub(r',', ' ', name)
        name = name.translate(str.maketrans('', '', PUNCT_TO_REMOVE))
        name = re.sub(r'\s+', ' ', name).strip()

        # Step 6: Remove duplicate words
        seen = set()
        result = []
        for word in name.split():
            if word not in seen:
                seen.add(word)
                result.append(word)
        name = ' '.join(result)

        if not name.strip():
            name = "None"

        return name


    #  if name has more than 2 words

    else :

        PUNCT_TO_REMOVE = string.punctuation

        # Step 1: Lowercase
        name = name.lower()

        # Step 2: Remove digits, URLs, specific Unicode chars   (remove but not replace with space)

        name = re.sub(r'\d+', '', name)
        name = re.sub(r'!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~', '', name)
        name = re.sub(r'(http|ftp|https)://[\S]+', '', name)
        name = re.sub('â€”', '', name)

        # Step 3: Remove unwanted keywords (like "d/o", "co", etc.)
        name = re.sub(r'\b(d[^\w\s]?o|c[^\w\s]?o|u[^\w\s]?g|s[^\w\s]?o|w[^\w\s]?o|do|co|ug|so|fng|fngof|mng|mngof|mngf|d o|c o|u g|s o|minor|lti|l t i|ung)\b[^,]*', '', name, flags=re.IGNORECASE)



        # Step 4: Remove multiple leading prefixes (like "late", "kumari", etc.)
        #name = re.sub(r'^(\s*(miss|late|lt|smt|mrs|mst|mr|kumari|ms|dr|kum|md|sk|lti)[^\w\s]*)+\s*','', name, flags=re.IGNORECASE)

        prefixes = r"(miss|late|lt|smt|mrs|mst|mr|kumari|kumar|ms|dr|kum|md|lti|master)"
        # Remove only the first prefix if it appears at the start
        name = re.sub(rf"^\s*{prefixes}\b\s*", '', name, flags=re.IGNORECASE)




        # Step 5: Remove punctuation and extra spaces
        name = re.sub(r',', ' ', name)
        name = name.translate(str.maketrans('', '', PUNCT_TO_REMOVE))
        name = re.sub(r'\s+', ' ', name).strip()

        # Step 6: Remove duplicate words
        name = name.lower()
        seen = set()
        result = []
        for word in name.split():
            if word not in seen:
                seen.add(word)
                result.append(word)
        name = ' '.join(result)

        if not name.strip():
            name = "None"

        return name


def process_dob(dob:str) -> str :
    x=dob
    x = re.sub('-','', x)
    return x

def convert_string(input_string):           # Removes all spaces from a string
  input_string = input_string.lower()
  return input_string.replace(" ", "")


def get_word_embeddings(name) :
  return model.encode(name)

# def get_data(file_path) :

df = pd.read_csv("xyz.csv")
df.columns = ['Applicant Id', 'Applicant Name', 'Father Name', 'Mother Name', 'Date of Birth']                    # rename

df.replace('', pd.NA, inplace=True)
df = df.dropna(subset=['Applicant Id', 'Applicant Name', 'Father Name', 'Mother Name', 'Date of Birth'])

df['Applicant Id']=df['Applicant Id'].astype(str)
df['Applicant Name']=df['Applicant Name'].astype(str)
df['Father Name']=df['Father Name'].astype(str)
df['Mother Name']=df['Mother Name'].astype(str)
df['Date of Birth']=df['Date of Birth'].astype(str)

df['Applicant Name'] = df.apply(lambda x: process_name(x['Applicant Name']), axis=1)
df['Father Name'] = df.apply(lambda x: process_name(x['Father Name']), axis=1)
df['Mother Name'] = df.apply(lambda x: process_name(x['Mother Name']), axis=1)
df['Date of Birth'] = df.apply(lambda x: process_dob(x['Date of Birth']), axis=1)

df['Applicant_Name_dense_embed'] = [get_word_embeddings(text) for text in df['Applicant Name']]
df['Father_Name_dense_embed'] = [get_word_embeddings(text) for text in df['Father Name']]
df['Mother_Name_dense_embed'] = [get_word_embeddings(text) for text in df['Mother Name']]
df['Date_of_Birth_dense_embed'] = [get_word_embeddings(text) for text in df['Date of Birth']]

df.head()

def create_collection(dimension, df, collection_name):

    # client = MilvusClient(
    #     url="http://localhost:19530",
    #     token="root:Milvus"
    # )

    client = MilvusClient("milvus_demo_1.db")              # optional



    schema = MilvusClient.create_schema(auto_id=False, enable_dynamic_field=True,)

    # schema.add_field(field_name="id", datatype=DataType.INT64, is_primary=True, auto_id=True)

    schema.add_field(field_name="Applicant_Id", datatype=DataType.VARCHAR, max_length=50,is_primary=True,auto_id=False)
    schema.add_field(field_name="Applicant_Name", datatype=DataType.VARCHAR, max_length=50)
    schema.add_field(field_name="Father_Name", datatype=DataType.VARCHAR, max_length=50)
    schema.add_field(field_name="Mother_Name", datatype=DataType.VARCHAR, max_length=50)
    schema.add_field(field_name="Date_of_Birth", datatype=DataType.VARCHAR, max_length=50)


    #schema.add_field(field_name="dense_embed", datatype=DataType.FLOAT_VECTOR, dim=dimension)
    schema.add_field(field_name="Applicant_Name_dense_embed", datatype=DataType.FLOAT_VECTOR, dim=dimension)
    schema.add_field(field_name="Father_Name_dense_embed", datatype=DataType.FLOAT_VECTOR, dim=dimension)
    schema.add_field(field_name="Mother_Name_dense_embed", datatype=DataType.FLOAT_VECTOR, dim=dimension)
    schema.add_field(field_name="Date_of_Birth_dense_embed", datatype=DataType.FLOAT_VECTOR, dim=dimension)


    # schema.add_field(field_name="Applicant_Name_dense_embed", datatype=DataType.FLOAT_VECTOR, dim=dimension,
    #                 index_params={"index_type":"HNSW", "metric_type":"COSINE", "params": {"M": 32,
    #                                                                                     "efConstruction": 100,
    #                                                                                     "quantization": {
    #                                                                                                         "type": "PQ",
    #                                                                                                         "params": {
    #                                                                                                                     "nbits": 8,
    #                                                                                                                     "nlist": 100  # optional, depending on your PQ config
    #                                                                                                                 }
    #                                                                                                         }
    #                                                                                     }
    #                             }
    #                 )

    # schema.add_field(field_name="Father_Name_dense_embed", datatype=DataType.FLOAT_VECTOR, dim=dimension,
    #                 index_params={"index_type":"HNSW", "metric_type":"COSINE", "params": {"M": 32,
    #                                                                                     "efConstruction": 100,
    #                                                                                     "quantization": {
    #                                                                                                         "type": "PQ",
    #                                                                                                         "params": {
    #                                                                                                                     "nbits": 8,
    #                                                                                                                     "nlist": 100  # optional, depending on your PQ config
    #                                                                                                                 }
    #                                                                                                         }
    #                                                                                     }
    #                             }
    #                 )


    # schema.add_field(field_name="Mother_Name_dense_embed", datatype=DataType.FLOAT_VECTOR, dim=dimension,
    #                 index_params={"index_type":"HNSW", "metric_type":"COSINE", "params": {"M": 32,
    #                                                                                     "efConstruction": 100,
    #                                                                                     "quantization": {
    #                                                                                                         "type": "PQ",
    #                                                                                                         "params": {
    #                                                                                                                     "nbits": 8,
    #                                                                                                                     "nlist": 100  # optional, depending on your PQ config
    #                                                                                                                 }
    #                                                                                                         }
    #                                                                                     }
    #                             }
    #                 )

    # schema.add_field(field_name="Date_of_Birth_dense_embed", datatype=DataType.FLOAT_VECTOR, dim=dimension,
    #                 index_params={"index_type":"HNSW", "metric_type":"COSINE", "params": {"M": 32,
    #                                                                                     "efConstruction": 100,
    #                                                                                     "quantization": {
    #                                                                                                         "type": "PQ",
    #                                                                                                         "params": {
    #                                                                                                                     "nbits": 8,
    #                                                                                                                     "nlist": 100  # optional, depending on your PQ config
    #                                                                                                                 }
    #                                                                                                         }
    #                                                                                     }
    #                             }
    #                 )




    index_params = client.prepare_index_params()

    # index_params.add_index(field_name="id")
    # index_params.add_index(field_name="applicant_id", index_type="INVERTED", )
    index_params.add_index(field_name="Applicant_Name", index_type="INVERTED", )
    index_params.add_index(field_name="Date_of_Birth", index_type="INVERTED", )
    index_params.add_index(field_name="Father_Name", index_type="INVERTED", )
    index_params.add_index(field_name="Mother_Name", index_type="INVERTED", )
    index_params.add_index(field_name="Applicant_Name_dense_embed", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},) # IVF FLAT
    index_params.add_index(field_name="Father_Name_dense_embed", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},)
    index_params.add_index(field_name="Mother_Name_dense_embed", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},)
    index_params.add_index(field_name="Date_of_Birth_dense_embed", index_type="IVF_FLAT",  metric_type="COSINE", params={'nlist':1024},)


    # index_params.add_index(field_name="Applicant_Name_dense_embed", index_type="HNSW", metric_type="COSINE")
    # index_params.add_index(field_name="Father_Name_dense_embed", index_type="HNSW", metric_type="COSINE")
    # index_params.add_index(field_name="Mother_Name_dense_embed", index_type="HNSW", metric_type="COSINE")
    # index_params.add_index(field_name="Date_of_Birth_dense_embed", index_type="HNSW", metric_type="COSINE")


    if client.has_collection(collection_name=collection_name):
        client.drop_collection(collection_name=collection_name)



    else:
        client.create_collection(
            collection_name=collection_name,
            schema=schema,
            index_params=index_params,

        )

        BATCH_SIZE = 10

        for start in range(0, len(df), BATCH_SIZE):
            end = start + BATCH_SIZE
            batch = df.iloc[start:end]

            # Convert batch to list of dictionaries, where each dictionary is a row
            data_to_insert = [
                        {
                            "Applicant_Id": str(row["Applicant Id"]),
                            "Applicant_Name": row["Applicant Name"],
                            "Father_Name": row["Father Name"],
                            "Mother_Name": row["Mother Name"],
                            "Date_of_Birth": str(row["Date of Birth"]),

                            "Applicant_Name_dense_embed": row["Applicant_Name_dense_embed"],
                            "Father_Name_dense_embed": row["Father_Name_dense_embed"],
                            "Mother_Name_dense_embed": row["Mother_Name_dense_embed"],
                            "Date_of_Birth_dense_embed": row["Date_of_Birth_dense_embed"],
                        }
                        for _, row in batch.iterrows()
                        ]

            print(f"Inserting rows {start} to {end} (total: {len(batch)})")

            #insert_result = client.insert(collection_name = collection_name,  data = data_to_insert)
            client.insert(collection_name = collection_name,  data = data_to_insert)

    #return 'insertion successful'
    return f"insertion successful and {len(df)} records inserted"

create_collection(dimension=384, df=df, collection_name="demo_collection")

def milvus_search(df, collection_name, threshold_milvus, threshold_levenstine, no_of_results):

    import csv
    from datetime import datetime

    # Prepare CSV file
    report_file = "milvus_results.csv"
    with open(report_file, "w", newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow([
            'input_applicant_id',
            'input_applicant_name',
            'input_father_name',
            'input_mother_name',
            'input_dob',
            'matched_applicant_id',
            'matched_applicant_name',
            'matched_father_name',
            'matched_mother_name',
            'matched_dob',
            'original_string',
            'matched_string',
            'milvus_score',
            'levenshtein_score'
        ])

    # Threshold conversion
    threshold_milvus = threshold_milvus / 100.0
    threshold_levenstine = threshold_levenstine / 100.0

    all_results = []

    for i in range(len(df)):

        input_row = df.iloc[i]

        # Concatenate input fields
        user_total = (
            convert_string(input_row['Applicant Name']) +
            convert_string(input_row['Father Name']) +
            convert_string(input_row['Mother Name']) +
            input_row['Date of Birth']
        )

        # Prepare search requests
        request_1 = AnnSearchRequest(
            data=[input_row['Applicant_Name_dense_embed']],
            anns_field="Applicant_Name_dense_embed",
            param={"metric_type": "COSINE", "nprobe": 10},
            limit=5
        )

        request_2 = AnnSearchRequest(
            data=[input_row['Father_Name_dense_embed']],
            anns_field="Father_Name_dense_embed",
            param={"metric_type": "COSINE", "nprobe": 10},
            limit=5
        )

        request_3 = AnnSearchRequest(
            data=[input_row['Mother_Name_dense_embed']],
            anns_field="Mother_Name_dense_embed",
            param={"metric_type": "COSINE", "nprobe": 10},
            limit=5
        )

        request_4 = AnnSearchRequest(
            data=[input_row['Date_of_Birth_dense_embed']],
            anns_field="Date_of_Birth_dense_embed",
            param={"metric_type": "COSINE", "nprobe": 10},
            limit=5
        )

        reqs = [request_1, request_2, request_3, request_4]

        # Hybrid search
        client = MilvusClient("milvus_demo_1.db")
        ranker = WeightedRanker(0.30, 0.30, 0.30, 0.10)

        results = client.hybrid_search(
            collection_name=collection_name,
            reqs=reqs,
            ranker=ranker,
            limit=5,
            output_fields=["Applicant_Id", "Applicant_Name", "Father_Name", "Mother_Name", "Date_of_Birth"]
        )

        for hits in results:

            #filtered = [hit for hit in hits if hit.distance > threshold_milvus]
            filtered = [hit for hit in hits if input_row['Applicant Id'] != hit.Applicant_Id and hit.distance > threshold_milvus]

            pro_corpus = [
                convert_string(hit.Applicant_Name) +
                convert_string(hit.Father_Name) +
                convert_string(hit.Mother_Name) +
                hit.Date_of_Birth for hit in filtered
            ]

            lev_scores = [
                dist.DamerauLevenshtein.normalized_similarity(user_total, matched)
                for matched in pro_corpus
            ]

            with open(report_file, "a", newline='', encoding='utf-8') as file:
                writer = csv.writer(file)

                for idx, (hit, lev_score) in enumerate(zip(filtered, lev_scores)):
                    if lev_score >= threshold_levenstine:
                        writer.writerow([
                            input_row['Applicant Id'],
                            input_row['Applicant Name'],
                            input_row['Father Name'],
                            input_row['Mother Name'],
                            input_row['Date of Birth'],
                            hit.Applicant_Id,
                            hit.Applicant_Name,
                            hit.Father_Name,
                            hit.Mother_Name,
                            hit.Date_of_Birth,
                            user_total,
                            pro_corpus[idx],
                            round(hit.distance * 100, 2),
                            round(lev_score * 100, 2)
                        ])

                        # Optional: store result in a list if needed programmatically
                        all_results.append({
                            'input_id': input_row['Applicant Id'],
                            'matched_id': hit.Applicant_Id,
                            'lev_score': lev_score,
                            'milvus_score': hit.distance
                        })

    return all_results


milvus_search(df=df, collection_name="demo_collection", threshold_milvus=40, threshold_levenstine=20, no_of_results=5)

# results = client.search(collection_name=collection_name,
    #                       data=[query_dense_vec],
    #                       anns_field="dense_embed_applicant_dob",


    #                       search_params={"params": {"ef":64},
    #                                     },

    #                       #search_params={"params": {"metric_type":"COSINE", "ef":1000}},     # HNSW



    #                       limit=5,
    #                       output_fields=["applicant_id", "applicant_name", "applicant_dob", "father_name", "mother_name"],
    #                       #filter=filter1,
    #                       #expr=expr1,
    #                       )


  #   for hits in results:
  #     # print(hits)
  #     # filtered = [hit for hit in hits if hit.distance > threshold_milvus]
  #     for hit in hits:


  #       if hit.distance>threshold_milvus:
  #         print(f"{df['applicant_id'][i]},{hit.applicant_id}, {df['applicant_full_name'][i]}, {hit.applicant_name}, {df['applicant_dob'][i]}, {hit.applicant_dob}, {df['applicant_father_full_name'][i]}, {hit.father_name}, {df['applicant_mother_full_name'][i]}, {hit.mother_name}, {hit.distance}")
  #         file.write(f"{df['applicant_id'][i]},{hit.applicant_id}, {df['applicant_full_name'][i]}, {hit.applicant_name}, {df['applicant_dob'][i]}, {hit.applicant_dob}, {df['applicant_father_full_name'][i]}, {hit.father_name}, {df['applicant_mother_full_name'][i]}, {hit.mother_name}, {hit.distance}\n")
  #     print("-----------------------------")

  #     # filtered_lst.append(filtered)

  # # return filtered_lst
  # file.close()





def milvus_search(df, collection_name, threshold_milvus) :

  report= "milvus_results.csv"
  file = open(report, "a")
  #file.write('applicant_id, matched_applicant_id, similarty_score\n')
  #file.write('applicant_id, matched_applicant_id, orginal, matched, similarty_score\n')
  file.write('applicant_id, matched_applicant_id, applicant_name, matched_applicant_name, applicant_dob, matched_applicant_dob, father_name, matched_father_name, mother_name, matched_mother_name, orginal, matched, similarty_score\n')
  filtered_lst = []

  for i in range(len(df)) :


    query_dense_vec = df['dense_embed_applicant_dob'].iloc[i]

    results = client.search(collection_name=collection_name,
                          data=[query_dense_vec],
                          anns_field="dense_embed_applicant_dob",


                          search_params={"params": {"ef":64},
                                        },

                          #search_params={"params": {"metric_type":"COSINE", "ef":1000}},     # HNSW



                          limit=5,
                          output_fields=["applicant_id", "applicant_name", "applicant_dob", "father_name", "mother_name"],
                          #filter=filter1,
                          #expr=expr1,
                          )


    for hits in results:
      # print(hits)
      # filtered = [hit for hit in hits if hit.distance > threshold_milvus]
      for hit in hits:


        if hit.distance>threshold_milvus:
          print(f"{df['applicant_id'][i]},{hit.applicant_id}, {df['applicant_full_name'][i]}, {hit.applicant_name}, {df['applicant_dob'][i]}, {hit.applicant_dob}, {df['applicant_father_full_name'][i]}, {hit.father_name}, {df['applicant_mother_full_name'][i]}, {hit.mother_name}, {hit.distance}")
          file.write(f"{df['applicant_id'][i]},{hit.applicant_id}, {df['applicant_full_name'][i]}, {hit.applicant_name}, {df['applicant_dob'][i]}, {hit.applicant_dob}, {df['applicant_father_full_name'][i]}, {hit.father_name}, {df['applicant_mother_full_name'][i]}, {hit.mother_name}, {hit.distance}\n")
      print("-----------------------------")

      # filtered_lst.append(filtered)

  # return filtered_lst
  file.close()



